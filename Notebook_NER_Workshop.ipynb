{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "99faa0ec",
      "metadata": {
        "id": "99faa0ec"
      },
      "source": [
        "# AI for Research: Customizing spaCy's Entity Recognition Models (Virtual)\n",
        "\n",
        "**Welcome to this interactive notebook!**  \n",
        "In this workshop, we'll walk through how Named Entity Recognition (NER) works, test pre-trained models, and learn how to customize them for research tasks, particularly useful in domains like hate speech detection, misinformation research, or media studies.\n",
        "\n",
        "\n",
        "### Prerequisites\n",
        "- Basic knowledge of Python\n",
        "- No prior experience with NER or spaCy required"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cpNOcd7Rbn4s",
      "metadata": {
        "id": "cpNOcd7Rbn4s"
      },
      "source": [
        "*This workshop was held on November 11, 2025, as part of the Research Computing and Data Services' **AI for Research** workshop series at Northwestern University, led by [Miriam Schirmer](https://miriamschirmer.github.io/).*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CUHzsnB-dYu6",
      "metadata": {
        "id": "CUHzsnB-dYu6"
      },
      "source": [
        "##**1. Introduction to Named Entity Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9684b7f",
      "metadata": {
        "id": "f9684b7f"
      },
      "source": [
        "### What is Named Entity Recognition?\n",
        "\n",
        "**NER** is a technique in Natural Language Processing (NLP) that identifies and classifies real-world entities in text.\n",
        "\n",
        "This sentence for example, has the following entities:\n",
        "\n",
        "*Dr. Jane Smith from the World Health Organization gave a talk in Geneva on July 15, 2021, about COVID-19.*\n",
        "\n",
        "- **PERSON** ‚Äì e.g., \"Dr. Jane Smith\"\n",
        "- **ORG** ‚Äì e.g., \"World Health Organization\"\n",
        "- **GPE** ‚Äì Geopolitical Entities, e.g., \"Geneva\"\n",
        "- **DATE** ‚Äì e.g., \"July 15, 2021\"\n",
        "- **Others** ‚Äì PRODUCT, EVENT, LAW, NORP (Nationalities or religious or political groups), etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cBvCQCDd0aY",
      "metadata": {
        "id": "4cBvCQCDd0aY"
      },
      "source": [
        "### Why is NER Important?\n",
        "\n",
        "NER helps:\n",
        "- Structure very raw and unformatted text, e.g., to get an overview of common terms used\n",
        "- Enable information extraction from social media, news, legal texts, etc.\n",
        "- Use it as an additional step for other NLP tasks (e.g., look at who is targeted when training a model to detect hate speech)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4054ada5",
      "metadata": {
        "id": "4054ada5"
      },
      "source": [
        "### Prep: Import relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20c3be6",
      "metadata": {
        "id": "d20c3be6"
      },
      "outputs": [],
      "source": [
        "# Import spaCy itself to build our pipeline: spaCy is the core NLP library we'll use.\n",
        "# spaCy provides pre-built pipelines for tasks like NER (and many more!).\n",
        "import spacy\n",
        "\n",
        "# Import \"Matcher\", which lets us define custom patterns\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Import \"EntityRuler\", which allows us to add custom rules for entities\n",
        "from spacy.pipeline import EntityRuler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80570ee8",
      "metadata": {
        "id": "80570ee8"
      },
      "source": [
        "### **Intro Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8MfceWo2-2cF",
      "metadata": {
        "id": "8MfceWo2-2cF"
      },
      "source": [
        "We‚Äôll start by loading **spaCy‚Äôs small English model**, called `en_core_web_sm`.\n",
        "\n",
        "- **`en_core_web_sm`** stands for *English (core) web-trained small model*.  \n",
        "  It includes the basic components of spaCy‚Äôs NLP pipeline: a tokenizer, part-of-speech tagger, dependency parser, and named entity recognizer.  \n",
        "- The **small model** is lightweight and fast, which makes it ideal for demos and teaching.  \n",
        "- For more accuracy (but slower performance), you can use:\n",
        "  - `en_core_web_md` ‚Üí *medium* model (includes word vectors)\n",
        "  - `en_core_web_lg` ‚Üí *large* model (best accuracy, higher memory use)\n",
        "- You can also train your own model or use models for other languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9840e2",
      "metadata": {
        "id": "8e9840e2"
      },
      "outputs": [],
      "source": [
        "# Load spaCy's small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "843c8407",
      "metadata": {
        "id": "843c8407"
      },
      "outputs": [],
      "source": [
        "# A simple text example\n",
        "text = \"Dr. Jane Smith from the World Health Organization gave a talk in Geneva on July 15, 2021, about COVID-19.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5685d7",
      "metadata": {
        "id": "2c5685d7"
      },
      "outputs": [],
      "source": [
        "# Run the NLP pipeline on the text\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750e2beb",
      "metadata": {
        "id": "750e2beb"
      },
      "outputs": [],
      "source": [
        "# Print entities detected by the model, including start and end character\n",
        "print(\"Entities Found:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"- {ent.text} ({ent.label_}) [Start: {ent.start_char}, End: {ent.end_char}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tkN-AvqI3_lF",
      "metadata": {
        "id": "tkN-AvqI3_lF"
      },
      "source": [
        "Let's visualize this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oA3kc8BP4DTX",
      "metadata": {
        "id": "oA3kc8BP4DTX"
      },
      "outputs": [],
      "source": [
        "# Visualize entities in the text\n",
        "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7953c99c",
      "metadata": {
        "id": "7953c99c"
      },
      "source": [
        "###**Excersise**: Use your own example and run this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "foiE4KSoenCs",
      "metadata": {
        "id": "foiE4KSoenCs"
      },
      "outputs": [],
      "source": [
        "# A simple text example (enter a sentence between the quotation marks)\n",
        "new_text = \"\"\n",
        "\n",
        "# ‚úèÔ∏è TODO: Enter a sentence between the quotation marks above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tFImWKtrfHLj",
      "metadata": {
        "id": "tFImWKtrfHLj"
      },
      "outputs": [],
      "source": [
        "# Run the NLP pipeline on the text\n",
        "new_doc =\n",
        "\n",
        "# ‚úèÔ∏è TODO: Run the NLP pipeline on the new text and store it in \"new_doc\" variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y2yO_MoKfOuN",
      "metadata": {
        "id": "y2yO_MoKfOuN"
      },
      "outputs": [],
      "source": [
        "# Print entities detected by the model, including start and end character\n",
        "print(\"Entities Found:\")\n",
        "for ent in new_doc.ents:\n",
        "    print(f\"- {ent.text} ({ent.label_}) [Start: {ent.start_char}, End: {ent.end_char}]\")\n",
        "\n",
        "# ‚úèÔ∏è TODO: No need to change this cell!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8948a9c5",
      "metadata": {
        "id": "8948a9c5"
      },
      "source": [
        "## **2. Applying NER to Real-World Data: Incel Forum Posts**\n",
        "\n",
        "Now that we've seen a basic example, let‚Äôs test how spaCy‚Äôs off-the-shelf NER performs on **incel forum posts**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80852124",
      "metadata": {
        "id": "80852124"
      },
      "source": [
        "### Background: What are \"incel\" forums?\n",
        "\n",
        "The term **incel** stands for \"involuntary celibate.\"  \n",
        "It refers to online communities where people discuss frustrations about dating and relationships, often expressing **hateful language toward women** and **misogynistic ideologies**.  \n",
        "\n",
        "Why are we using this data?\n",
        "- They use **slang and community-specific terms** that are different from everyday language but also contain **clear references to people, groups, and institutions**\n",
        "- They provide examples of **messy, real-world text** where standard NLP models may struggle.\n",
        "- They are publicly available data often used in research on online communities.\n",
        "\n",
        "This is especially relevant for **hate speech detection research**, where:\n",
        "- Extracting entities helps identify targeted individuals or groups.\n",
        "- We may want to track mentions of public figures, communities, or ideologies.\n",
        "\n",
        "‚ö†Ô∏è In this workshop, we use incel forum text **only as an example** to show how NER works on social science data.  \n",
        "Our focus is on the **method (NER)**, not on the community or its views.\n",
        "\n",
        "üö® Content warning: Incel terminology often contains misogynistic expressions and may reference sexual or gender-based violence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a153e5b2",
      "metadata": {
        "id": "a153e5b2"
      },
      "source": [
        "### üìÇ Step 1: Load Dataset and Inspect `text` Column\n",
        "\n",
        "We have a dataset (CSV file) of incel posts with a column called `text`. This column contains the raw text of each post made in a forum.\n",
        "\n",
        "We'll load the data, inspect a few entries, and then apply spaCy's NER model to extract named entities.\n",
        "\n",
        "üîç This mimics a typical hate speech or social media dataset structure. Note that this is **raw, unprocessed data**. It‚Äôs intentionally left messy to illustrate the kinds of challenges you might face when applying NER, and to show how to clean and prepare your data for this task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ebb611",
      "metadata": {
        "id": "b3ebb611"
      },
      "outputs": [],
      "source": [
        "# Import the pandas library for working with tables (dataframes)\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OA75MtXdgMr0",
      "metadata": {
        "id": "OA75MtXdgMr0"
      },
      "outputs": [],
      "source": [
        "# URL of the CSV file on GitHub to read it directly into a pandas DataFrame\n",
        "url = \"https://raw.githubusercontent.com/MiriamSchirmer/Intro-to-NER/refs/heads/main/incel_comments.csv\"\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "# A DataFrame is like a table (rows = observations, columns = variables)\n",
        "\n",
        "df = pd.read_csv(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ae3298",
      "metadata": {
        "id": "10ae3298"
      },
      "outputs": [],
      "source": [
        "# Set pandas option to display the full content of the 'text' column\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Display the first 5 rows of the dataset to check what it looks like\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8YEItp6jgeEB",
      "metadata": {
        "id": "8YEItp6jgeEB"
      },
      "outputs": [],
      "source": [
        "# Display the shape of the DataFrame to see how many rows (first number) and columns (second number) we have\n",
        "print(\"Shape of the DataFrame:\")\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94970fdc",
      "metadata": {
        "id": "94970fdc"
      },
      "source": [
        "### üè∑ Step 2: Apply NER to the `text` Column\n",
        "\n",
        "Now we apply the NER pipeline to each post in the dataset.  \n",
        "We‚Äôll extract:\n",
        "- The full list of entities\n",
        "- Their labels (e.g., PERSON, ORG)\n",
        "- Their frequency in the dataset\n",
        "\n",
        "This helps us:\n",
        "- Spot key actors and targets in hate speech\n",
        "- Identify misclassifications (e.g., slang detected as ORG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5c4fe9c",
      "metadata": {
        "id": "f5c4fe9c"
      },
      "source": [
        "Extract Named Entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b68a864",
      "metadata": {
        "id": "2b68a864"
      },
      "outputs": [],
      "source": [
        "# Define a function that takes in a piece of text and returns all named entities the model finds\n",
        "\n",
        "def extract_ents(text, nlp):\n",
        "    \"\"\"\n",
        "    Extract named entities from a given text using a spaCy pipeline.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text from which to extract entities.\n",
        "        nlp (spacy.language.Language): A loaded spaCy language model (e.g., spacy.load(\"en_core_web_sm\")).\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: A list containing (entity_text, entity_label) pairs.\n",
        "    \"\"\"\n",
        "    # Process the text through the provided spaCy pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Collect each entity as a (text, label) pair\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    return entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974f1f27",
      "metadata": {
        "id": "974f1f27"
      },
      "outputs": [],
      "source": [
        "# Define what our texts are\n",
        "texts = df['text']\n",
        "\n",
        "# Process all texts efficiently in batches (this saves time because we are not calling the model separately for each row)\n",
        "# You can adjust batch_size depending on text length\n",
        "docs = list(nlp.pipe(texts, batch_size=10)) # low batch size because our posts are rather short\n",
        "\n",
        "# Extract entities for each processed doc\n",
        "df['entities'] = [[(ent.text, ent.label_) for ent in doc.ents] for doc in docs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3cd830",
      "metadata": {
        "id": "de3cd830"
      },
      "outputs": [],
      "source": [
        "# Show 15 random rows with text and extracted entities\n",
        "df[['text', 'entities']].sample(15, random_state=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41858a95",
      "metadata": {
        "id": "41858a95"
      },
      "source": [
        "Count Most Common Entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e15bd3",
      "metadata": {
        "id": "88e15bd3"
      },
      "outputs": [],
      "source": [
        "# Import Counter, a helper tool that counts how often items appear in a list\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de697605",
      "metadata": {
        "id": "de697605"
      },
      "outputs": [],
      "source": [
        "# Flatten (i.e., \"unzip\")the list of entities across all posts\n",
        "# - df['entities'] contains one list of entities per row\n",
        "# - We loop over each row and then each entity inside it\n",
        "all_entities = [ent for row in df['entities'] for ent in row]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b4e80e",
      "metadata": {
        "id": "e5b4e80e"
      },
      "outputs": [],
      "source": [
        "# Count how often each (text, label) pair appears in the dataset\n",
        "entity_counter = Counter(all_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43af9570",
      "metadata": {
        "id": "43af9570"
      },
      "outputs": [],
      "source": [
        "# Print the 15 most frequent named entities\n",
        "print(\"Most Frequent Named Entities:\")\n",
        "for (text, label), count in entity_counter.most_common(15):\n",
        "    print(f\"{text} ({label}): {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QGDm--HskQNc",
      "metadata": {
        "id": "QGDm--HskQNc"
      },
      "source": [
        "Adapt this slightly to exclude the numbers (which we are not really interested in for our current usecase)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qs4JY9sIkPPX",
      "metadata": {
        "id": "Qs4JY9sIkPPX"
      },
      "outputs": [],
      "source": [
        "exclude_labels = {\"CARDINAL\", \"ORDINAL\", \"PERCENT\"}\n",
        "\n",
        "# Filter the entities to exclude those with labels in exclude_labels\n",
        "filtered_entities = [ent for ent in all_entities if ent[1] not in exclude_labels]\n",
        "\n",
        "# Count how often each (text, label) pair appears in the filtered list\n",
        "entity_counter = Counter(filtered_entities)\n",
        "\n",
        "# Print the 15 most frequent named entities from the filtered list\n",
        "print(\"Most Frequent Named Entities (excluding numbers):\")\n",
        "for (text, label), count in entity_counter.most_common(15):\n",
        "    print(f\"{text} ({label}): {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GLF1hEgti8UF",
      "metadata": {
        "id": "GLF1hEgti8UF"
      },
      "source": [
        "###**Discussion**: What works well, what does not work well? How can we improve this?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa8db14",
      "metadata": {
        "id": "4fa8db14"
      },
      "source": [
        "### What We Can Learn from the Entity Counts\n",
        "\n",
        "- Did the model mark any words as entities that **aren‚Äôt actually entities**?  \n",
        "- Are the **real people or names** we care about being tagged correctly?  \n",
        "- Are there **important words** that the model missed?\n",
        "\n",
        "These questions help us see what needs to be improved, either by:\n",
        "- **Fixing** specific cases with simple rules (using the `EntityRuler`)\n",
        "- **Teaching** the model new examples through training\n",
        "\n",
        "Next, we‚Äôll look at how to **customize and improve** the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617982af",
      "metadata": {
        "id": "617982af"
      },
      "source": [
        "## üß© Step 3: Why Customize NER?\n",
        "\n",
        "spaCy's default model doesn't recognize many **domain-specific concepts** in incel communities.\n",
        "\n",
        "Examples:\n",
        "- ‚ÄúChad‚Äù, ‚ÄúStacy‚Äù ‚Üí Often central figures, not recognized as people\n",
        "- ‚ÄúTinder‚Äù, ‚ÄúReddit‚Äù ‚Üí Should be detected as platforms\n",
        "- ‚ÄúRedpill‚Äù, ‚ÄúBlackpill‚Äù ‚Üí Ideologies\n",
        "- ‚Äúnormie‚Äù, ‚Äúfoid‚Äù  ‚Üí Community-specific terms\n",
        "\n",
        "Let‚Äôs start by using **spaCy‚Äôs Matcher** and **EntityRuler** to inject these into the pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef227cb5",
      "metadata": {
        "id": "ef227cb5"
      },
      "source": [
        "####üíª Customizing Option A: Rule-Based Matching with `Matcher`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549d7690",
      "metadata": {
        "id": "549d7690"
      },
      "outputs": [],
      "source": [
        "# Create a Matcher object, which lets us define custom rules\n",
        "# It needs the vocabulary (nlp.vocab) from the spaCy model\n",
        "matcher = Matcher(nlp.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33039d61",
      "metadata": {
        "id": "33039d61"
      },
      "outputs": [],
      "source": [
        "# Define a simple pattern for the word \"chad\"\n",
        "# LOWER means: match the lowercase version of the token\n",
        "pattern_chad = [{\"LOWER\": \"chad\"}]\n",
        "\n",
        "# Define a pattern for the word \"stacy\"\n",
        "pattern_stacy = [{\"LOWER\": \"stacy\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c708d37",
      "metadata": {
        "id": "9c708d37"
      },
      "outputs": [],
      "source": [
        "# Add both patterns to the matcher under the same label \"INCEL_PERSON\"\n",
        "# The first argument (\"INSEL_PERSON\") is the name we give this rule\n",
        "# The second argument is a list of patterns we want to match\n",
        "matcher.add(\"INCEL_PERSON\", [pattern_chad, pattern_stacy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a14a9a1",
      "metadata": {
        "id": "0a14a9a1"
      },
      "outputs": [],
      "source": [
        "# Store counts\n",
        "match_counter = Counter()\n",
        "total_matches = 0\n",
        "\n",
        "# Loop through your dataframe\n",
        "for doc in nlp.pipe(df[\"text\"], batch_size=50):\n",
        "    matches = matcher(doc)\n",
        "    total_matches += len(matches)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        label = nlp.vocab.strings[match_id]  # e.g. \"INCEL_PERSON\"\n",
        "        span_text = doc[start:end].text\n",
        "        match_counter[(span_text, label)] += 1\n",
        "\n",
        "# Summary\n",
        "print(f\"Total matches found: {total_matches}\\n\")\n",
        "\n",
        "print(\"Most Frequent Matcher Hits:\")\n",
        "for (text, label), count in match_counter.most_common():\n",
        "    print(f\"{text} ({label}): {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_GBX6tnEnXwJ",
      "metadata": {
        "id": "_GBX6tnEnXwJ"
      },
      "source": [
        "Let's look at an example that contains \"Stacy\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u4lvDwCKnLbs",
      "metadata": {
        "id": "u4lvDwCKnLbs"
      },
      "outputs": [],
      "source": [
        "# Find a text entry that contains \"Stacy\"\n",
        "stacy_text = df[df['text'].str.contains('Stacy', case=False, na=False)].iloc[0]['text']\n",
        "\n",
        "# Print the text\n",
        "print(\"Example text containing 'Stacy':\")\n",
        "print(stacy_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HHIjm4UEwdne",
      "metadata": {
        "id": "HHIjm4UEwdne"
      },
      "source": [
        "###**Excersise**: Choose a term you would like to add and run the the NER Matcher on our dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0STRMO7loNfG",
      "metadata": {
        "id": "0STRMO7loNfG"
      },
      "outputs": [],
      "source": [
        "# Define your own pattern(s)\n",
        "\n",
        "new_pattern = [{\"LOWER\": \"\"}]\n",
        "matcher.add(\"\", [new_pattern])\n",
        "\n",
        "# ‚úèÔ∏è TODO: Replace the underscores above with your own term and label! Replace \"YOUR_LABEL_HERE\" with your label name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I4crDq5NpBQ-",
      "metadata": {
        "id": "I4crDq5NpBQ-"
      },
      "outputs": [],
      "source": [
        "# Count how often we find your new pattern! (No adjustments needed.)\n",
        "\n",
        "# Store counts\n",
        "match_counter = Counter()\n",
        "total_matches = 0\n",
        "\n",
        "# Loop through your dataframe\n",
        "for doc in nlp.pipe(df[\"text\"], batch_size=50):\n",
        "    matches = matcher(doc)\n",
        "    total_matches += len(matches)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        label = nlp.vocab.strings[match_id]  # e.g. \"INCEL_PERSON\"\n",
        "        span_text = doc[start:end].text\n",
        "        match_counter[(span_text, label)] += 1\n",
        "\n",
        "# Summary\n",
        "print(f\"Total matches found: {total_matches}\\n\")\n",
        "\n",
        "print(\"Most Frequent Matcher Hits:\")\n",
        "for (text, label), count in match_counter.most_common():\n",
        "    print(f\"{text} ({label}): {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85759833",
      "metadata": {
        "id": "85759833"
      },
      "source": [
        "###üíª Customizing Option B: Insert Custom Entities with `EntityRuler`\n",
        "\n",
        "The **EntityRuler** is similar to the Matcher, but with one key difference:\n",
        "\n",
        "- **Matcher**: Finds patterns in text but does not automatically turn them into \"entities\".  \n",
        "  ‚Üí We had to manually print the matches.  \n",
        "\n",
        "- **EntityRuler**: Lets us directly insert new *named entities* into spaCy‚Äôs pipeline.  \n",
        "  ‚Üí The matches will appear alongside other entities (like PERSON, ORG, DATE) when we run `doc.ents`.\n",
        "\n",
        "This makes the EntityRuler a better choice if we want our custom rules to behave just like the built-in NER model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O6ZYAMQLtPnO",
      "metadata": {
        "id": "O6ZYAMQLtPnO"
      },
      "source": [
        "Reset the NLP Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1odVYOVBtHFv",
      "metadata": {
        "id": "1odVYOVBtHFv"
      },
      "outputs": [],
      "source": [
        "# Start fresh to avoid lingering patterns/rulers\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Remove any existing entity_ruler(s)\n",
        "for name in list(nlp.pipe_names):\n",
        "    if name.startswith(\"entity_ruler\"):\n",
        "        nlp.remove_pipe(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "im9ANQWgtbgh",
      "metadata": {
        "id": "im9ANQWgtbgh"
      },
      "outputs": [],
      "source": [
        "# Add a NEW entity_ruler with lowercased phrase matching and overwrite behavior\n",
        "ruler = nlp.add_pipe(\n",
        "    \"entity_ruler\",\n",
        "    before=\"ner\",\n",
        "    config={\"overwrite_ents\": True, \"phrase_matcher_attr\": \"LOWER\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KX1enNZOtf1U",
      "metadata": {
        "id": "KX1enNZOtf1U"
      },
      "source": [
        "Now we are adding our new labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jV-TR8aAtzv6",
      "metadata": {
        "id": "jV-TR8aAtzv6"
      },
      "outputs": [],
      "source": [
        "# Option one: Define precise patterns, here for platforms:\n",
        "platforms = [\"Tinder\", \"Reddit\", \"YouTube\", \"Instagram\", \"TikTok\", \"Twitter\", \"X\"]\n",
        "patterns = [{\"label\": \"PLATFORM\", \"pattern\": p} for p in platforms]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B_eN7anAt3Dp",
      "metadata": {
        "id": "B_eN7anAt3Dp"
      },
      "outputs": [],
      "source": [
        "# Option two: Other custom entities\n",
        "patterns += [\n",
        "    {\"label\": \"PERSON\",   \"pattern\": \"Chad\"},\n",
        "    {\"label\": \"PERSON\",   \"pattern\": \"Stacy\"},\n",
        "    {\"label\": \"IDEOLOGY\", \"pattern\": \"Redpill\"},\n",
        "    {\"label\": \"IDEOLOGY\", \"pattern\": \"Blackpill\"},\n",
        "    {\"label\": \"COMMUNITY\",\"pattern\": \"normie\"},\n",
        "    {\"label\": \"SLUR\",     \"pattern\": \"foid\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ZKB0-R6t-CL",
      "metadata": {
        "id": "5ZKB0-R6t-CL"
      },
      "outputs": [],
      "source": [
        "# We add the patterns\n",
        "ruler.add_patterns(patterns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYOOyalRuBDT",
      "metadata": {
        "id": "EYOOyalRuBDT"
      },
      "source": [
        "Let's look at the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6402c0e",
      "metadata": {
        "id": "a6402c0e"
      },
      "outputs": [],
      "source": [
        "# Count entities on your dataset\n",
        "exclude_labels = {\"CARDINAL\", \"ORDINAL\", \"PERCENT\"}\n",
        "entity_counter = Counter()\n",
        "\n",
        "for doc in nlp.pipe(df[\"text\"], batch_size=50):\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ not in exclude_labels:\n",
        "            entity_counter[(ent.text, ent.label_)] += 1\n",
        "\n",
        "print(\"Most Frequent Named Entities (EntityRuler + NER):\")\n",
        "for (text, label), count in entity_counter.most_common(20):\n",
        "    print(f\"{text} ({label}): {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sw2nuC95uP37",
      "metadata": {
        "id": "Sw2nuC95uP37"
      },
      "source": [
        "###**Excersise**: Choose a term you would like to add and run the the NER Matcher on our dataset! Use the code above to add your examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c72a81",
      "metadata": {
        "id": "01c72a81"
      },
      "source": [
        "## üìö **3. Additional Material: Training a Custom NER Model (Simple Demo)**\n",
        "\n",
        "So far, we‚Äôve used:\n",
        "- Pre-trained entities (PERSON, ORG, DATE, etc.)\n",
        "- Rule-based customization (Matcher, EntityRuler)\n",
        "\n",
        "Another option is to **train the model** to recognize new entity types.  \n",
        "This requires **annotated data**, i.e., examples of text with entity spans labeled.\n",
        "\n",
        "‚ö†Ô∏è This is just a toy demo to show the mechanics. Real training needs more data and time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a6a16f0",
      "metadata": {
        "id": "2a6a16f0"
      },
      "source": [
        "### Train the model from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QR7shG9X6uRl",
      "metadata": {
        "id": "QR7shG9X6uRl"
      },
      "source": [
        "### **Understanding Model Training in spaCy**\n",
        "\n",
        "Before training our own **Named Entity Recognition (NER)** model, here are the key ideas to understand:\n",
        "\n",
        "\n",
        "\n",
        "##### **Key Concepts**\n",
        "\n",
        "| üè∑Ô∏è **Concept** | üí° **What it Means** | üéØ **Why it Matters** |\n",
        "|:----------------|:--------------------|:----------------------|\n",
        "| **Annotated data** | Training needs examples where entities are *already labeled* in text, e.g. `\"Redpill\" ‚Üí IDEOLOGY`. | The model can only learn from what it sees.<br><br>More examples and variety = better generalization. |\n",
        "| **Empty model**<br/>`spacy.blank(\"en\")` | Creates a model with **no prior knowledge** (\"a clean slate\"). | Ideal for demos or custom domains.<br><br>The model learns entirely from the input data. |\n",
        "| **Pretrained model**<br/>`en_core_web_sm` | A model that already understands **general English** syntax and entities. | You can **fine-tune** it instead of training from scratch.<br><br>This saves time and requires less data. |\n",
        "| **Adding an NER component** | spaCy pipelines are sequences like:<br/>`tokenizer ‚Üí tagger ‚Üí parser ‚Üí NER`. | Adding an NER step lets the model detect and label entities (e.g., `PERSON`, `ORG`, or custom ones like `IDEOLOGY`). |\n",
        "| **Token alignment & BILUO tags** | spaCy internally converts entity spans into the **BILUO** format:<br>**B**egin, **I**nside, **L**ast, **U**nit, **O**utside. | Ensures that entity spans match token boundaries.<br><br>This alignment is **essential for error-free training**. |\n",
        "| **Epochs / iterations** | One ‚Äúepoch‚Äù = one **full pass** through the dataset. Training repeats over multiple epochs. | Each pass helps the model refine its understanding.<br><br>More epochs ‚Üí more learning (to a point). |\n",
        "| **Updating model weights** | After every batch, spaCy adjusts internal **weights** based on the difference between predictions and correct labels. | These updates make the model gradually improve.<br><br>Over many updates, accuracy and stability increase. |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2XKEsY9c0mCC",
      "metadata": {
        "id": "2XKEsY9c0mCC"
      },
      "source": [
        "**Notes**\n",
        "\n",
        "\n",
        "* This is a toy example. With only a few sentences, the model will overfit quickly; that‚Äôs fine for demonstration.\n",
        "* For deterministic terms (exact names), an EntityRuler is often a better choice. Use training for fuzzier/variable mentions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vdVrnUH-BddK",
      "metadata": {
        "id": "vdVrnUH-BddK"
      },
      "source": [
        "Let's start the training!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MmYzDj6LBfhN",
      "metadata": {
        "id": "MmYzDj6LBfhN"
      },
      "source": [
        "Here's a summary of what the following code does:\n",
        "\n",
        "\n",
        "1. **Build a tiny training set** for a Named Entity Recognition (NER) task using a helper function that ensures entity spans align correctly to tokens\n",
        "(this prevents \"misalignment\" errors during training)\n",
        "2. Create and **train** a completely blank English NER **model** from scratch on two custom labels: IDEOLOGY and PLATFORM\n",
        "3. **Evaluate** the trained model on a new test sentence to see if it learned to recognize similar patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kZmdkd6gCDUK",
      "metadata": {
        "id": "kZmdkd6gCDUK"
      },
      "outputs": [],
      "source": [
        "# Import libraries for text processing and model training\n",
        "import re\n",
        "import random\n",
        "import spacy\n",
        "from spacy.training import Example, offsets_to_biluo_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t_AxIixUCF20",
      "metadata": {
        "id": "t_AxIixUCF20"
      },
      "source": [
        "Define a helper function to build token-aligned entity spans. The `make_example()` function ensures that entity spans (start and end positions) line up exactly with token boundaries. This is required by spaCy for training.\n",
        "\n",
        "If an entity span cuts through a token (e.g., due to punctuation or whitespace), it will raise a clear error so you can adjust the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bkUVGPKCqCN",
      "metadata": {
        "id": "5bkUVGPKCqCN"
      },
      "outputs": [],
      "source": [
        "def make_example(text, spans, nlp_for_tokenization=None):\n",
        "    \"\"\"\n",
        "    text: str -> the input sentence\n",
        "    spans: list of tuples -> [(substring, LABEL), ...]\n",
        "           e.g. [(\"Blackpill\", \"IDEOLOGY\")]\n",
        "    Finds the FIRST occurrence of each substring in `text`,\n",
        "    checks that it aligns to token boundaries, and returns\n",
        "    a tuple in the format spaCy expects: (text, {\"entities\": [(start, end, LABEL), ...]})\n",
        "    \"\"\"\n",
        "    nlp_tok = nlp_for_tokenization or spacy.blank(\"en\")\n",
        "    doc = nlp_tok.make_doc(text)\n",
        "    ents = []\n",
        "    for substr, label in spans:\n",
        "        m = re.search(re.escape(substr), text)\n",
        "        if not m:\n",
        "            raise ValueError(f\"Substring not found: {substr!r} in: {text!r}\")\n",
        "        start_char, end_char = m.start(), m.end()\n",
        "        if doc.char_span(start_char, end_char) is None:\n",
        "            # If this happens, the substring doesn‚Äôt match full tokens.\n",
        "            # You can fix this by adjusting the substring or the tokenizer.\n",
        "            tokens = [t.text for t in doc]\n",
        "            raise ValueError(\n",
        "                f\"Not token-aligned: {substr!r} -> ({start_char},{end_char}). \"\n",
        "                f\"Tokens: {tokens}\"\n",
        "            )\n",
        "        ents.append((start_char, end_char, label))\n",
        "    return (text, {\"entities\": ents})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "InBloeQICuts",
      "metadata": {
        "id": "InBloeQICuts"
      },
      "source": [
        "1) Build a tiny toy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wsn3nb2bC7X_",
      "metadata": {
        "id": "wsn3nb2bC7X_"
      },
      "outputs": [],
      "source": [
        "# Each entry is created with make_example() to ensure safe alignment.\n",
        "# The data has two labels: \"IDEOLOGY\" (e.g., Blackpill, Redpill) and \"PLATFORM\" (e.g., Reddit, Tinder).\n",
        "\n",
        "TRAIN_DATA = [\n",
        "    make_example(\"He follows the Blackpill ideology.\", [(\"Blackpill\", \"IDEOLOGY\")]),\n",
        "    make_example(\"Redpill beliefs are common on these forums.\", [(\"Redpill\", \"IDEOLOGY\")]),\n",
        "    make_example(\"She spends time on Reddit.\", [(\"Reddit\", \"PLATFORM\")]),\n",
        "    make_example(\"They met through Tinder.\", [(\"Tinder\", \"PLATFORM\")]),\n",
        "    make_example(\"Many users argue about Blackpill ideas on Reddit.\",\n",
        "                 [(\"Blackpill\", \"IDEOLOGY\"), (\"Reddit\", \"PLATFORM\")]),\n",
        "    make_example(\"Tinder and Reddit are popular apps.\",\n",
        "                 [(\"Tinder\", \"PLATFORM\"), (\"Reddit\", \"PLATFORM\")]),\n",
        "]\n",
        "\n",
        "\n",
        "# Optional: quick sanity check for alignment\n",
        "# -----------------------------------------------------\n",
        "# The function below visualizes tokenization and entity alignment\n",
        "# by converting entities into the BILUO tagging scheme.\n",
        "# BILUO = Begin, Inside, Last, Unit, Outside\n",
        "# Misaligned entities will appear as '-' in the sequence.\n",
        "\n",
        "def check_alignment(text, ents):\n",
        "    doc = spacy.blank(\"en\").make_doc(text)\n",
        "    print(text)\n",
        "    print(\"TOKENS:\", [t.text for t in doc])\n",
        "    print(\"BILUO:\", offsets_to_biluo_tags(doc, ents), \"\\n\")\n",
        "\n",
        "for text, ann in TRAIN_DATA:\n",
        "    check_alignment(text, ann[\"entities\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xFR_7v2cC9Lb",
      "metadata": {
        "id": "xFR_7v2cC9Lb"
      },
      "source": [
        "2) Create a blank NER pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FgvEBBq6DCvT",
      "metadata": {
        "id": "FgvEBBq6DCvT"
      },
      "outputs": [],
      "source": [
        "# Start from an empty English pipeline and add the NER component.\n",
        "# Register the custom labels so the model knows what to predict.\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "ner.add_label(\"IDEOLOGY\")\n",
        "ner.add_label(\"PLATFORM\")\n",
        "\n",
        "# Initialize training parameters (weights and optimizer)\n",
        "optimizer = nlp.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7nkFeBZhDEjV",
      "metadata": {
        "id": "7nkFeBZhDEjV"
      },
      "source": [
        "3) Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ELzKyX-ADe85",
      "metadata": {
        "id": "ELzKyX-ADe85"
      },
      "outputs": [],
      "source": [
        "# For demonstration purposes, we train for a small number of iterations on a very small dataset.\n",
        "# This is NOT a realistic setup ‚Äî it‚Äôs just to showhow the model learns to recognize the two entity types.\n",
        "\n",
        "# Fix the random seed for reproducible results\n",
        "random.seed(42)\n",
        "\n",
        "# Number of training iterations (epochs)\n",
        "n_iter = 15\n",
        "\n",
        "for i in range(n_iter):\n",
        "    # Shuffle training examples each epoch\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "\n",
        "    # Train on each text‚Äìannotation pair\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        doc = nlp.make_doc(text)\n",
        "        example = Example.from_dict(doc, annotations)\n",
        "        nlp.update([example], sgd=optimizer, losses=losses, drop=0.2)\n",
        "\n",
        "    # Show progress every 5 epochs\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Iteration {i} | Losses: {losses}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkr5jvrIDgNF",
      "metadata": {
        "id": "wkr5jvrIDgNF"
      },
      "source": [
        "4) Test the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fjHrMekVzA1F",
      "metadata": {
        "id": "fjHrMekVzA1F"
      },
      "outputs": [],
      "source": [
        "# Try the model on a new sentence that combines both entity types\n",
        "# to see if it generalizes beyond the training examples.\n",
        "\n",
        "test_text = \"People debate Redpill ideas on Reddit and meet on Tinder.\"\n",
        "doc = nlp(test_text)\n",
        "print(\"\\nTest text:\", test_text)\n",
        "print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c0d608",
      "metadata": {
        "id": "e9c0d608"
      },
      "source": [
        "## üéØ What We Learned\n",
        "\n",
        "- Pre-trained NER is a great *starting point*, but‚Ä¶\n",
        "- Social media / hate speech data has **slang and unique entities** that default models miss.\n",
        "- Rule-based methods (`Matcher`, `EntityRuler`) let us quickly adapt NER for research.\n",
        "- Combining **default + rules + fine-tuning** makes the strongest pipelines.\n",
        "\n",
        "For your projects: Think about which entities are most meaningful (people? platforms? ideologies?) and adapt NER accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aHWVZI9LEdVa",
      "metadata": {
        "id": "aHWVZI9LEdVa"
      },
      "source": [
        "## üìö Further Resources: Named Entity Recognition (NER)\n",
        "\n",
        "If you‚Äôd like to explore Named Entity Recognition further ‚Äî especially in the context of customization, domain adaptation, or research use ‚Äî here are some carefully selected resources:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y0KVZN9REkT1",
      "metadata": {
        "id": "Y0KVZN9REkT1"
      },
      "source": [
        "### üß™ Tutorials and Beginner-Friendly Guides\n",
        "\n",
        "- **spaCy Course (Highly Recommended)**  \n",
        "  https://course.spacy.io  \n",
        "  Interactive tutorials on NER, rule-based matching, and building pipelines.\n",
        "\n",
        "- **NLTK Book ‚Äì Chapter 7: Information Extraction**  \n",
        "  https://www.nltk.org/book/ch07.html  \n",
        "  Classic introduction to NER using rule-based techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jnnq0Wu1EmNS",
      "metadata": {
        "id": "Jnnq0Wu1EmNS"
      },
      "source": [
        "### üß† Customizing and Training NER Models with spaCy\n",
        "\n",
        "- **spaCy NER Docs**  \n",
        "  https://spacy.io/usage/linguistic-features#named-entities  \n",
        "  Overview of how NER works in spaCy and how to access entity labels.\n",
        "\n",
        "- **spaCy Rule-Based Matching** (Matcher & EntityRuler)  \n",
        "  https://spacy.io/usage/rule-based-matching  \n",
        "  How to define token patterns and add custom entities.\n",
        "\n",
        "- **Training a Custom NER Model in spaCy**  \n",
        "  https://spacy.io/usage/training  \n",
        "  End-to-end guide to creating training data and training your own model.\n",
        "\n",
        "- **Using spaCy Projects for Training Pipelines**  \n",
        "  https://spacy.io/usage/projects  \n",
        "  Helps manage training configs, assets, and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V9J9H27YEtcm",
      "metadata": {
        "id": "V9J9H27YEtcm"
      },
      "source": [
        "### ü§ñ Alternative NER Frameworks\n",
        "\n",
        "- **Hugging Face Transformers (for Fine-Tuned NER Models)**  \n",
        "  https://huggingface.co/models?pipeline_tag=token-classification  \n",
        "  Browse pre-trained NER models like `bert-base-cased-finetuned-conll03`.\n",
        "\n",
        "- **Tutorial: Fine-Tuning BERT for NER (Hugging Face)**  \n",
        "  https://huggingface.co/transformers/v4.6.1/custom_datasets.html#named-entity-recognition  \n",
        "  Advanced tutorial using PyTorch and Hugging Face datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be81eb17",
      "metadata": {
        "id": "be81eb17"
      },
      "source": [
        "\n",
        "### üìÑ (Some) Key Papers and Benchmarks\n",
        "\n",
        "\n",
        "- **Tjong Kim Sang & De Meulder (2003)**  \n",
        "  [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition](https://aclanthology.org/W03-0419/)  \n",
        "  *Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003*.\n",
        "\n",
        "- **Nadeau & Sekine (2007)**  \n",
        "  [A Survey of Named Entity Recognition and Classification](https://www.jbe-platform.com/content/journals/10.1075/li.30.1.03nad)  \n",
        "  *Lingvisticae Investigationes, 30(1), 3-26*\n",
        "\n",
        "- **Li et al. (2022)**  \n",
        "  [A Survey on Deep Learning for Named Entity Recognition](https://ieeexplore.ieee.org/abstract/document/9039685)  \n",
        "  *IEEE Transactions on Knowledge and Data Engineering, 2021*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
